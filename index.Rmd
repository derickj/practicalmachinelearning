---
title: "Practical Machine Learning Course Project"
author: "Derick"
date: "18 December 2015"
output: html_document
---

## Executive Summary



## Exploratory Analysis

```{r loadlibs}
library(caret)
library(rattle)
```

The training data file was loaded and a series of exploratory analyses performed. [UGUL2012] was referenced to udnerstand the structure of the data set.  It was found that a number of the variables are unlikely to be relevant to be used in prediction, i.e. the identifying information such as X, the name of the user, timestamps and window information (these variables were columns 1-7 of the data).  Further, it was found that for a large number of columns, there were no or almost no valid observations.  These columns (where NAs consituted more than 90% of observations) were eliminated from the study.  The input file labelled some observations as character, even though the variable represented numeric values, and these were coerced to numeric.  Subsequent to these transformations, 52 potential predictor variables and the "classe" variable remained in the data set.

The same transformations were performed to the test data set in order to ensure consistency before the final testing of the predictions generated by the chosen prediction model.


```{r loadandread}
trainfile <- "pml-training.csv"
if(!file.exists(trainfile)) {
    download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",trainfile)    
}
training <- read.csv(trainfile,stringsAsFactors=FALSE,na.strings=c("NA","","#DIV/0"))
training$classe <- as.factor(training$classe)

testfile <- "pml-testing.csv"
if(!file.exists(testfile)) {
    download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv",testfile)
}
testing <- read.csv(testfile,stringsAsFactors=FALSE,na.strings=c("NA","","#DIV/0"))

# Drop the initial columns not useful to the prediction
training <- training[,8:160]
testing <- testing[,8:159] # For testing the problem id column at the end is also dropped
cutoff <- 0.9*nrow(training)
selcols <- which(colSums(is.na(training)) < cutoff)
training <- training[, selcols]
testing <- testing[, selcols[1:(length(selcols)-1)]]
```

## Data splitting and preprocessing

The training data set is split into a training and a test (validation) data set, with 70% observations allocated to training and the rest to the validation set.  All model development is done using the training data only, and then validated against the 30% test data.

The supplied test data set is only used in the final prediction to be submitted as part of the project.

```{r splitandprocess}
set.seed(8668)
intrain <- createDataPartition(y = training$classe, p = 0.7, list = FALSE)
mytrain <- training[intrain, ]
mytest <- training[-intrain, ]
```

## Alternative prediction models

### Decision tree

```{r decisiontree}
modfit1 <- train (classe ~ ., method="rpart", data = mytrain)
fancyRpartPlot(modfit1$finalModel)
testpredict <- predict(modfit1, newdata = mytest[,-53])
confusionMatrix(testpredict,mytest$classe)
```

The classification tree did not result in a very good prediction, less than 50% accuracy!

### Generalized Boosted Regression Model

Next, we fit a generalized boosted regression model.

``` {r gbm, cache = TRUE}
modfit2 <- train (classe ~ ., method="gbm", data = mytrain, verbose = FALSE)
testpredict <- predict(modfit2, newdata = mytest[,-53])
confusionMatrix(testpredict,mytest$classe)
```

The GBM provides much improved accuracy over the classification and regression tree.

### Random Forest

Finally, the random forest method is used on the training data to build a third model.

``` {r randomforest, cache = TRUE}
modfit3 <- train (classe ~ ., method="rf", data = mytrain)
testpredict <- predict(modfit3, newdata = mytest[,-53])
confusionMatrix(testpredict,mytest$classe)
```

## Conclusion

The accuracy of the random forest prediction when applied to the validation data set is the best of the alternative models developed.

## Out of Sample Error

## Predict test cases 

```{r testcases}
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

# Create submission files
answers <- predict(modfit3, newdata = testing)
# Predict Answers using model from first part
pml_write_files(answers)
```

## References

[UGUL2012] Ugulino, W.; Cardador, D.; Vega, K.; Velloso, E.; Milidiu, R.; Fuks, H. Wearable Computing: Accelerometers' Data Classification of Body Postures and Movements. Proceedings of 21st Brazilian Symposium on Artificial Intelligence. Advances in Artificial Intelligence - SBIA 2012. In: Lecture Notes in Computer Science. , pp. 52-61. Curitiba, PR: Springer Berlin / Heidelberg, 2012. ISBN 978-3-642-34458-9. DOI: 10.1007/978-3-642-34459-6_6. 
