---
title: "Using Sensor Data to Classify Weightlifting Movements Qualitatively"
author: "derickj predmachlearn-035"
date: "19 December 2015"
output: html_document
---

## Executive Summary

The study uses data from research conducted by [UGUL2012] into the classification of correct and incorrect movements used by study subjects doing weightlifting exercises using sensor data.  Sensor data was collected for a number of subjects performing the exercises in five different (correct and incorrect) ways.  The training data set provides detail sensor data together with the classification of the way in which the exercise was performed as A,B,C,D or E.

In this paper, the data was used to develop a predictive model which could be applied to sensor data to determine the correct or incorrect way an exercise is perfomed, the ulitmate goal being to provide realtime feedback to a user of a device which monitors personal activity.

The data was analysed and transformed, with 70% of the training data used to develop a number of alternative predictive models. Each model was cross-validated to the remaining 30% of the training data to determine how accurate it is.  It was found that the random forest model performed better than a decision tree of generalized boosted regression model and was used to predict the types of movements of the test data set.

## Exploratory Analysis

```{r loadlibs}
library(caret)
library(rattle)
```

The training data file is loaded and a series of exploratory analyses were performed. [UGUL2012] was referenced to determine the structure of the data set, since no codebook was provided with the data.  It was found that a number of the variables are unlikely to be relevant to be used in prediction, i.e., the identifying information such as X, the name of the user, timestamps and window information (these variables were columns 1-7 of the data).  Further, it was found that for a large number of columns, there were no or almost no valid observations.  These columns (where NAs consituted more than 90% of observations) were eliminated from the study.  The input file labelled some observations as character, even though the variable represented numeric values, and these were coerced to numeric.  Subsequent to these transformations, 52 potential predictor variables and the "classe" variable classifying the 5 ways of doing the exercises remained in the data set.

The same transformations were performed to the test data set in order to ensure consistency before using the chosen prediction model to predict the class of exercise by the observations contained in the test data set.

```{r loadandread}
trainfile <- "pml-training.csv"
if(!file.exists(trainfile)) {
    download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",trainfile)    
}
training <- read.csv(trainfile,stringsAsFactors=FALSE,na.strings=c("NA","","#DIV/0"))
training$classe <- as.factor(training$classe)

testfile <- "pml-testing.csv"
if(!file.exists(testfile)) {
    download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv",testfile)
}
testing <- read.csv(testfile,stringsAsFactors=FALSE,na.strings=c("NA","","#DIV/0"))

# Drop the initial columns not useful to the prediction
training <- training[,8:160]
testing <- testing[,8:159] # For testing the problem id column at the end is also dropped
cutoff <- 0.9*nrow(training)
selcols <- which(colSums(is.na(training)) < cutoff)
training <- training[, selcols]
testing <- testing[, selcols[1:(length(selcols)-1)]]
```

## Data Splitting and Preprocessing

The training data set is split into a training and a test (validation) data set, with 70% observations allocated to training and the rest to the validation set (mytest).  All model development was done using the reduced training data only (mytrain), and then validated against the 30% test data (mytest).

The supplied test data set (testing) is only used in the final prediction to be submitted as part of the project.

```{r splitandprocess}
set.seed(8668)
intrain <- createDataPartition(y = training$classe, p = 0.7, list = FALSE)
mytrain <- training[intrain, ]
mytest <- training[-intrain, ]
featurePlot(x=mytrain[,c("roll_belt","roll_forearm","magnet_dumbbell_y")],y=mytrain$classe,plot="pairs")
par(mfrow = c(2, 2), mar = c(4, 4, 4, 1))
qplot(roll_belt,colour=classe,data=mytrain,geom="density")
qplot(magnet_dumbbell_y,colour=classe,data=mytrain,geom="density")
qplot(roll_forearm,colour=classe,data=mytrain,geom="density")
qplot(accel_belt_z,colour=classe,data=mytrain,geom="density")
```

## Alternative Predictive Models

In the following, alternative prediction models are developed and valdiated against the 'mytest' data to determine the accuracy of the model.

### Decision Tree

```{r decisiontree}
modfit1 <- train (classe ~ ., method="rpart", data = mytrain)
fancyRpartPlot(modfit1$finalModel)
testpredict <- predict(modfit1, newdata = mytest[,-53])
confusionMatrix(testpredict,mytest$classe)
```

The classification tree did not result in a very good prediction, with less than 50% accuracy.

### Generalized Boosted Regression Model

Next, a generalized boosted regression model is fitted.

``` {r gbm, cache = TRUE}
modfit2 <- train (classe ~ ., method="gbm", data = mytrain, verbose = FALSE)
testpredict <- predict(modfit2, newdata = mytest[,-53])
confusionMatrix(testpredict,mytest$classe)
```

The generalized boosted regression model provides much improved accuracy over the classification and regression tree, with more than 96% accuracy when used on the 'mytest' observations.

### Random Forest

Finally, the random forest method is used on the training data to build a third model. The default trainControl parameters for the "rf" method is to do bootstrapping with 25 repetitions of resampling.  This was sufficient to generate quite an accurate model.   With more computing resources and time, it could be interesting to determine whether more repetitions or multiple cross-validation deliver an even better model.

``` {r randomforest, cache = TRUE}
modfit3 <- train (classe ~ ., method="rf", data = mytrain)
testpredict <- predict(modfit3, newdata = mytest[,-53])
confusionMatrix(testpredict,mytest$classe)
varImp(modfit3)
plot(modfit3, main="Random Forest Model")
```

## Out of Sample Error

For each of the above mentioned models, the model was developed using 70% of the original training data, with the model then cross validated by predicting the values of the remaining 30% observations using each model and comparing with the actual classe variable as observed.  The confusionMatrix calculated the various test statistics, with accuracy being the key one to determine out-of-sample error. For the random forest model, accuracy was 99.05%, with a resultant out-of-sample error of less than 1%.

## Conclusion

The accuracy of the random forest prediction model when applied to the validation data set (mytrain) was the best of the alternative models developed.

## References

[UGUL2012] Ugulino, W.; Cardador, D.; Vega, K.; Velloso, E.; Milidiu, R.; Fuks, H. Wearable Computing: Accelerometers' Data Classification of Body Postures and Movements. Proceedings of 21st Brazilian Symposium on Artificial Intelligence. Advances in Artificial Intelligence - SBIA 2012. In: Lecture Notes in Computer Science. , pp. 52-61. Curitiba, PR: Springer Berlin / Heidelberg, 2012. ISBN 978-3-642-34458-9. DOI: 10.1007/978-3-642-34459-6_6. 

## Annexure - Predict Test Cases 

The orginal test set (having been transformed with the same transformations applied to the original training data) is used to predict the classe variable (predict which of the 5 types of correct and incorrect methods were used during the exercise represented by each row's observations). Output files are generated to be submitted for the second part of the assignment. 

```{r testcases}
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}
# Predict Answers using the best model from first part
answers <- predict(modfit3, newdata = testing)
answers
# Create submission files
pml_write_files(answers)
```

